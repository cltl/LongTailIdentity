{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import regex as re\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import utils\n",
    "import en_coref_md\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gvdb_articles_file = '../../gvdb-aggregated-db/Articles-with-extracted-info.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se_annotation_file='../data/input/partial/annotation/participants_input.p'\n",
    "se_text_file='../data/input/partial/text/docs.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load coreference model & resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_offsets(doc):\n",
    "    sent_offsets=[]\n",
    "    for s in doc.sents:\n",
    "        sent_offsets.append(tuple([s.start_char, s.end_char]))\n",
    "    return sent_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6), (7, 16), (17, 29)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt ='Hello. I am you. Who are you?'\n",
    "doc = nlp(txt)\n",
    "get_sentence_offsets(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emiel_resources_file='../resources/emiel.json'\n",
    "with open (emiel_resources_file, 'r') as f:\n",
    "    emiel_resources=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "students_resources_file='../resources/students.json'\n",
    "with open (students_resources_file, 'r') as f:\n",
    "    students_resources=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CauseOfDeath', 'Residence', 'EducationLevel', 'Ethnicity', 'Religion', 'BirthPlace', 'PastConviction'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_resources.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attributes=['age', 'race', 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_most_frequent_words(rdr, attribute):\\n    frequency_count=defaultdict(int)\\n    for row in rdr:\\n        data=json.loads(row[3])\\n        for s in sections:\\n            for participant in data[s]:\\n                if participant[attribute][\\'value\\']:\\n                    value=participant[attribute][\\'value\\'].strip()\\n                    frequency_count[value]+=1\\n    return Counter(frequency_count).most_common(50)\\n    \\nfrequency_count={}\\nwith open(gvdb_articles_file, \\'r\\') as csvfile:\\n    rdr = csv.reader(csvfile, delimiter=\\'\\t\\', quotechar=\\'\"\\')\\n    header=next(rdr)\\n    frequency_count[\\'race\\']=get_most_frequent_words(rdr, \\'race\\')\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def get_most_frequent_words(rdr, attribute):\n",
    "    frequency_count=defaultdict(int)\n",
    "    for row in rdr:\n",
    "        data=json.loads(row[3])\n",
    "        for s in sections:\n",
    "            for participant in data[s]:\n",
    "                if participant[attribute]['value']:\n",
    "                    value=participant[attribute]['value'].strip()\n",
    "                    frequency_count[value]+=1\n",
    "    return Counter(frequency_count).most_common(50)\n",
    "    \n",
    "frequency_count={}\n",
    "with open(gvdb_articles_file, 'r') as csvfile:\n",
    "    rdr = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
    "    header=next(rdr)\n",
    "    frequency_count['race']=get_most_frequent_words(rdr, 'race')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_races = set(emiel_resources['ethnicity'])\n",
    "tmp_races |= set({'black', 'latino', 'white', 'hispanic', 'asian', 'latina', \n",
    "       'african american', 'filipino', 'african-american', 'latinos',\n",
    "      'palestinian', 'chinese-american', 'blacks', 'german-iranian'})\n",
    "tmp_races |= set(students_resources['Ethnicity'])\n",
    "\n",
    "races={o:o for o in tmp_races}\n",
    "\n",
    "\n",
    "#genders={'male': {'he', 'boy', 'man', 'dude', 'guy', 'male'}, 'female': {'girl', 'woman', 'female'}}\n",
    "\n",
    "genders={'he': 'male', \n",
    "         'boy': 'male', \n",
    "         'man': 'male', \n",
    "         'dude': 'male',\n",
    "         'guy': 'male',\n",
    "         'male': 'male',\n",
    "         'brother': 'male',\n",
    "         'father': 'male',\n",
    "         'him': 'male',\n",
    "         'himself': 'male',\n",
    "         'she': 'female',\n",
    "         'girl': 'female',\n",
    "         'woman': 'female',\n",
    "         'female': 'female',\n",
    "         'sister': 'female',\n",
    "         'mother': 'female',\n",
    "         'her': 'female',\n",
    "         'herself': 'female'}\n",
    "\n",
    "occupations={o:o for o in emiel_resources['occupation-or-social-group']}\n",
    "\n",
    "#religions={o:o for o in (emiel_resources['religion']+students_resources['Religion'])}\n",
    "religions={o:o for o in students_resources['Religion']}\n",
    "\n",
    "educations=students_resources['EducationLevel']\n",
    "del educations['a']\n",
    "\n",
    "convictions=students_resources['PastConviction']\n",
    "\n",
    "causes=students_resources['CauseOfDeath']\n",
    "\n",
    "age_patterns=[r'\\d\\d?-year-old', r', \\d\\d?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attribute value extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Proximity based extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attr_values_extractor(text, people_spans, coref_spans, sentence_offsets, patterns, a_dict=None):\n",
    "    \"\"\"Generic extractor that operates based on patterns.\"\"\"\n",
    "    \n",
    "    extracted_pairs=defaultdict(list)\n",
    "    for pattern in patterns:\n",
    "        r=re.compile(pattern, re.IGNORECASE)\n",
    "        values=r.finditer(text)\n",
    "        for val_found in values:\n",
    "            span=val_found.span()\n",
    "            value=val_found.group()\n",
    "            value=value.replace('-year-old', '').replace(',', '').strip()\n",
    "            if a_dict and value in a_dict:\n",
    "                value=a_dict[value]\n",
    "            person, distance=utils.find_closest_person(span, \n",
    "                                                       people_spans, \n",
    "                                                       coref_spans, \n",
    "                                                       sentence_offsets,\n",
    "                                                       min_dist=1000)\n",
    "            if person:\n",
    "                extracted_pairs[person].append(tuple([distance, value]))\n",
    "    clean_pairs=utils.get_closest_value_per_person(extracted_pairs)\n",
    "    return clean_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pattern_extractor(text, people_spans, coref_spans, sentence_offsets, patterns=None, pattern_data=None):\n",
    "    if patterns: # if there are patterns given, fire the function immediately\n",
    "        return attr_values_extractor(text, people_spans, coref_spans, sentence_offsets, patterns, pattern_data)\n",
    "    \n",
    "    #else create them first\n",
    "    patterns=set()\n",
    "    cs=set(pattern_data.keys())\n",
    "    for o in cs:\n",
    "        patterns.add(r'\\b%s\\b' % o)\n",
    "    return attr_values_extractor(text, people_spans, coref_spans, sentence_offsets, patterns, pattern_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Coreference based extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attr_extractor_coref(clusters, names, values_json, debug=False): #, text, people_spans):\n",
    "    \n",
    "    person_data=defaultdict(list)\n",
    "\n",
    "    if not clusters:\n",
    "        return person_data\n",
    "    \n",
    "    for c in clusters:\n",
    "        mentions=utils.stringify_cluster_mentions(c.mentions)\n",
    "        for person_name in names:\n",
    "            if utils.lookup_person_in_list(person_name, mentions):\n",
    "                for m in c.mentions:\n",
    "                    for txt in [m.text, m.lemma_]:\n",
    "                        if txt.lower() in values_json.keys():\n",
    "                            person_data[person_name].append(values_json[txt.lower()])\n",
    "                            \n",
    "    clean_data={}\n",
    "    for person_name, gs in person_data.items():\n",
    "        c=Counter(gs).most_common(1)[0][0]\n",
    "        clean_data[person_name]=c\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Run extraction of all properties for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_properties(names, full_text, nlp):\n",
    "    \n",
    "    doc = nlp(full_text)\n",
    "    # run coreference\n",
    "    people_spans, coref_spans, clusters=utils.get_coref_spans(names, full_text, doc)\n",
    "\n",
    "    sentence_offsets=utils.get_sentence_offsets(doc)\n",
    "    \n",
    "    # run individual extractors\n",
    "    gender_extracted=attr_extractor_coref(clusters, names, genders)\n",
    "#    gender_extracted=pattern_extractor(full_text,people_spans, coref_spans, pattern_data=genders)\n",
    "    \n",
    "    age_extracted=pattern_extractor(full_text, people_spans, coref_spans, sentence_offsets, patterns=age_patterns)\n",
    "    \n",
    "    race_extracted=pattern_extractor(full_text,people_spans, coref_spans, sentence_offsets, pattern_data=races)\n",
    "#    race_extracted=attr_extractor_coref(clusters, names, races)\n",
    "    \n",
    "    occupation_extracted=pattern_extractor(full_text, people_spans, coref_spans, sentence_offsets, pattern_data=occupations)\n",
    "    #attr_extractor_coref(clusters, names, occupations)\n",
    "    #print(occupation_extracted)\n",
    "    \n",
    "    religion_extracted=pattern_extractor(full_text, people_spans, coref_spans, sentence_offsets, pattern_data=religions)\n",
    "#    religion_extracted=attr_extractor_coref(clusters, names, religions)\n",
    "    #if religion_extracted:\n",
    "    #    print(religion_extracted)\n",
    "    \n",
    "    education_extracted=pattern_extractor(full_text, people_spans, coref_spans, sentence_offsets, pattern_data=educations)\n",
    "    #education_extracted=attr_extractor_coref(clusters, names, educations)\n",
    "    #if education_extracted:\n",
    "    #    print(education_extracted)\n",
    "        \n",
    "    conviction_extracted=pattern_extractor(full_text, people_spans, coref_spans, sentence_offsets, pattern_data=convictions)\n",
    "    #conviction_extracted=attr_extractor_coref(clusters, names, convictions)\n",
    "    #if conviction_extracted:\n",
    "    #    print(conviction_extracted)\n",
    "    \n",
    "    causes_extracted=pattern_extractor(full_text, people_spans, coref_spans, sentence_offsets, pattern_data=causes)\n",
    "    #if causes_extracted:\n",
    "    #    print(causes_extracted)  \n",
    "    \n",
    "    all_extracted={'age': age_extracted, 'ethnicity': race_extracted, 'gender': gender_extracted, \n",
    "                  'religion': religion_extracted, 'occupation': occupation_extracted, \n",
    "                   'educationlevel': education_extracted, 'causeofdeath': causes_extracted,\n",
    "                  'pastconviction': conviction_extracted}\n",
    "    \n",
    "    #combine extractors\n",
    "    combined=utils.singularize_data(all_extracted)\n",
    "            \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Peter Boer': {'age': '26',\n",
       "  'causeofdeath': 'intentional',\n",
       "  'ethnicity': 'white',\n",
       "  'occupation': 'police'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test stuff\n",
    "\n",
    "txt='Hello Peter Boer, 26 was shot. The white police guy failed.'\n",
    "names=['Peter Boer']\n",
    "extract_properties(names, txt, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run GVDB extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_gvdb_data(the_file, limit=200):\n",
    "\n",
    "    all_gold_rows=[]\n",
    "    all_sys_rows=[]\n",
    "\n",
    "    with open(the_file, 'r') as csvfile:\n",
    "        rdr = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
    "        header=next(rdr)\n",
    "        \n",
    "        for c, row in enumerate(rdr):\n",
    "            if c==limit: break\n",
    "\n",
    "            full_text=row[2]\n",
    "            data=json.loads(row[3])\n",
    "            part_info=utils.get_participant_info(data)\n",
    "\n",
    "            names=set(part_info.keys())\n",
    "            if not len(names): continue\n",
    "\n",
    "            system_data = extract_properties(names, full_text, nlp)\n",
    "            all_sys_rows.append(system_data)\n",
    "            all_gold_rows.append(part_info)\n",
    "            \n",
    "            c+=1\n",
    "    return all_sys_rows, all_gold_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_sys_rows, all_gold_rows = process_gvdb_data(gvdb_articles_file, limit=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run SE extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conll_to_text(rows):\n",
    "    tokens=[]\n",
    "    for row in rows:\n",
    "        elements=row.split('\\t')\n",
    "#        print(elements, elements[1])\n",
    "        if elements[1]=='NEWLINE':\n",
    "            elements[1]='\\n'\n",
    "        tokens.append(elements[1])\n",
    "    text=' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_names(part_data):\n",
    "    names=[]\n",
    "    for part_id, part_info in part_data.items():\n",
    "        if 'Name' in part_info.keys() and part_info['Name'].strip():\n",
    "            names.append(part_info['Name'].strip())\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_part_info(data):\n",
    "    new_data={}\n",
    "    for part_id, part in data.items():\n",
    "        if 'Name' in part and part['Name'].strip():\n",
    "            name=part['Name'].strip()\n",
    "            del part['Name']\n",
    "            new_part={}\n",
    "            for k,v in part.items():\n",
    "                k=k.strip().lower()\n",
    "                new_part[k]=v.strip().lower()\n",
    "            new_data[name]=new_part\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_se_data(annotation_file, text_file, limit=200):\n",
    "    all_gold_rows=[]\n",
    "    all_sys_rows=[]\n",
    "\n",
    "    with open(annotation_file, 'rb') as af:\n",
    "        annotations=pickle.load(af)\n",
    "    \n",
    "    with open(text_file, 'rb') as tf:\n",
    "        all_texts_json=pickle.load(tf)\n",
    "        \n",
    "    cnt=0\n",
    "    for doc_id, part_data in annotations.items():\n",
    "\n",
    "        names=get_names(part_data)\n",
    "        \n",
    "        if doc_id not in all_texts_json:\n",
    "            continue\n",
    "        text_json=all_texts_json[doc_id]\n",
    "        conll_data=text_json['content']\n",
    "        text=conll_to_text(conll_data)\n",
    "        \n",
    "        properties=extract_properties(names, text, nlp)\n",
    "        \n",
    "        all_sys_rows.append(properties)\n",
    "        \n",
    "        part_info=transform_part_info(part_data)\n",
    "        all_gold_rows.append(part_info)\n",
    "        \n",
    "        cnt+=1\n",
    "        if cnt==limit: break\n",
    "        \n",
    "    return all_sys_rows, all_gold_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_system, se_gold = process_se_data(se_annotation_file, se_text_file, limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(se_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devin Anderson african american\n",
      "Ahmad Antoine african american\n",
      "Alejandro Martinez hispanic/latin\n",
      "Bryant Sanchez white/caucascian\n",
      "Stanley Greene african american\n",
      "Luis Canseco hispanic/latin\n",
      "Lydell McLaurin african american\n",
      "James Graham african american\n",
      "Gary Holmes african american\n",
      "Tyre King african american\n",
      "defaultdict(<class 'int'>, {'residence': 403, 'age': 739, 'gender': 756, 'educationlevel': 117, 'causeofdeath': 488, 'deathplace': 565, 'deathdate': 565, 'birthplace': 6, 'pastconviction': 32, 'religion': 17, 'medicalcondition': 8, 'ethnicity': 10})\n"
     ]
    }
   ],
   "source": [
    "count_per_attr=defaultdict(int)\n",
    "for row in se_gold:\n",
    "    for part, data in row.items():\n",
    "        for attr in data.keys():\n",
    "            count_per_attr[attr]+=1\n",
    "            if attr=='ethnicity':\n",
    "                print(part, data[attr])\n",
    "print(count_per_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Benchmark extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark_extractors(system, gold, attributes, debug=False):\n",
    "    assert len(system)==len(gold)\n",
    "    tp=defaultdict(int)\n",
    "    fp=defaultdict(int)\n",
    "    fn=defaultdict(int)\n",
    "    \n",
    "    \n",
    "    for index, gold_row in enumerate(gold):\n",
    "        system_row=system[index]\n",
    "        \n",
    "        for part, gold_vals in gold_row.items():\n",
    "            try:\n",
    "                system_vals=system_row[part]\n",
    "            except KeyError:\n",
    "                system_vals={}\n",
    "            \n",
    "            for a in attributes:\n",
    "                gold_val=''\n",
    "                system_val=''\n",
    "                if a in gold_vals:\n",
    "                    gold_val=gold_vals[a].strip()\n",
    "                if a in system_vals:\n",
    "                    system_val=system_vals[a].strip()\n",
    "                if gold_val and system_val:\n",
    "                    if gold_val==system_val:\n",
    "                        tp[a]+=1\n",
    "                    else:\n",
    "                        fp[a]+=1\n",
    "                        fn[a]+=1\n",
    "                elif gold_val:\n",
    "                    fn[a]+=1\n",
    "                elif system_val:\n",
    "                    fp[a]+=1\n",
    "    \n",
    "    recall={}\n",
    "    prec={}\n",
    "    f1={}\n",
    "    \n",
    "    print(tp,fp, fn)\n",
    "    for a in attributes:\n",
    "        prec[a]=tp[a]/(tp[a]+fp[a])\n",
    "        recall[a]=tp[a]/(tp[a]+fn[a])\n",
    "        f1[a]=2*prec[a]*recall[a]/(prec[a]+recall[a])\n",
    "    return prec, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Benchmark GVDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_sys_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-76ea92def311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark_extractors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sys_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_gold_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_sys_rows' is not defined"
     ]
    }
   ],
   "source": [
    "benchmark_extractors(all_sys_rows, all_gold_rows, attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Benchmark on SemEval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se_attributes=['age', 'gender', 'pastconviction', 'educationlevel', \n",
    "               'causeofdeath', 'ethnicity', 'religion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'teen 12-17'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-ed19c92c3d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msys_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'age'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mse_attributes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mse_system\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_age_to_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mse_system\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-ed19c92c3d78>\u001b[0m in \u001b[0;36mmap_age_to_group\u001b[0;34m(sys_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'age'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mage_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'teen 12-17'"
     ]
    }
   ],
   "source": [
    "def map_age_to_group(sys_data):\n",
    "    for doc in sys_data:\n",
    "        for part, data in doc.items():\n",
    "            if 'age' in data:\n",
    "                age=int(data['age'])\n",
    "                age_group=None\n",
    "                if age<12:\n",
    "                    age_group='child 0-11'\n",
    "                elif age<18:\n",
    "                    age_group='teen 12-17'\n",
    "                elif age<65:\n",
    "                    age_group='adult 18-64'\n",
    "                else:\n",
    "                    age_group='senior 65+'\n",
    "                data['age']=age_group\n",
    "    return sys_data\n",
    "if 'age' in se_attributes:\n",
    "    se_system=map_age_to_group(se_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'age': 192, 'educationlevel': 23, 'gender': 103, 'causeofdeath': 87, 'pastconviction': 8}) defaultdict(<class 'int'>, {'pastconviction': 76, 'educationlevel': 14, 'causeofdeath': 136, 'age': 14, 'religion': 4, 'ethnicity': 16, 'gender': 14}) defaultdict(<class 'int'>, {'age': 547, 'gender': 653, 'causeofdeath': 401, 'educationlevel': 94, 'pastconviction': 24, 'religion': 17, 'ethnicity': 10})\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-033a6225a850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark_extractors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mse_system\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_gold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-32d959c61885>\u001b[0m in \u001b[0;36mbenchmark_extractors\u001b[0;34m(system, gold, attributes, debug)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "benchmark_extractors(se_system, se_gold, se_attributes, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## * Next steps:\n",
    "* benchmark\n",
    "* other properties: religion, school, deathplace, residence, birthplace, occupation,\n",
    "* evaluate extrinsically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
