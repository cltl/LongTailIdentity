{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Prepare to run\n",
    "\n",
    "#### 1.1 Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import existing ones\n",
    "import csv\n",
    "import regex as re\n",
    "from collections import defaultdict, Counter\n",
    "import en_coref_md\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules we created\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Set data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: set which partition to work with!\n",
    "which_partition='partial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SemEval input files (annotations and text)\n",
    "\n",
    "# annotation data\n",
    "se_annotation_dir='../data/input/%s/annotation' % which_partition\n",
    "se_annotation_file='%s/participants_input.p' % se_annotation_dir\n",
    "\n",
    "# text documents\n",
    "se_partial_text_file='../data/input/partial/text/docs.p'\n",
    "se_all_documents_path='../data/input/full/text'\n",
    "\n",
    "# SemEval output paths\n",
    "se_output_dir='extracted_data/%s' % which_partition\n",
    "se_output_file='%s/extracted_data.p' % se_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GVDB data file\n",
    "gvdb_articles_file = '../../gvdb-aggregated-db/Articles-with-extracted-info.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Load lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emiel_resources_file='../resources/emiel.json'\n",
    "emiel_resources=utils.load_json(emiel_resources_file)\n",
    "\n",
    "students_resources_file='../resources/students.json'\n",
    "students_resources=utils.load_json(students_resources_file)\n",
    "\n",
    "genders_resources_file='../resources/genders.json'\n",
    "genders=utils.load_json(genders_resources_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Load coreference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Define mapping of properties (lower-to-capitalletter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_properties ={\n",
    "                    'age': 'Age', \n",
    "                    'ethnicity': 'Ethnicity',\n",
    "                    #'occupation': 'Occupation',\n",
    "                    'causeofdeath': 'CauseOfDeath',\n",
    "                    'religion': 'Religion',\n",
    "                    'educationlevel': 'EducationLevel',\n",
    "                    'pastconviction': 'PastConviction',\n",
    "                    'residence': 'Residence',\n",
    "                    'birthplace': 'BirthPlace',\n",
    "                    'gender': 'Gender'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_attributes=list(map_properties.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Define how to process which attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process based on patterns and keywords\n",
    "pattern_attrs=['age','ethnicity', 'religion', 'educationlevel', \n",
    "               'causeofdeath', 'pastconviction', 'birthplace', 'residence']\n",
    "\n",
    "# Process based on coreference\n",
    "coref_attrs=['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Prepare the lexicons per attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Prepare pattern keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_data=defaultdict(dict)\n",
    "for k,v in students_resources.items():\n",
    "    new_v=utils.remove_stopwords(v)\n",
    "    pattern_data[k.lower()]=new_v\n",
    "\n",
    "#pattern_data['occupation']={o:o for o in emiel_resources['occupation-or-social-group']}\n",
    "\n",
    "#pattern_data['ethnicity'] |= set(emiel_resources['ethnicity'])\n",
    "#pattern_data['ethnicity'] |= set({'black', 'latino', 'white', 'hispanic', 'asian', 'latina', \n",
    "#       'african american', 'filipino', 'african-american', 'latinos',\n",
    "#      'palestinian', 'chinese-american', 'blacks', 'german-iranian'})\n",
    "\n",
    "\n",
    "#religions={o:o for o in (emiel_resources['religion']+students_resources['Religion'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 Prepare patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exact patterns per attribute\n",
    "patterns={'age': [r'\\d\\d?-year-old', r', \\d\\d?']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr, keyword_json in pattern_data.items(): \n",
    "    attr_patterns=set()\n",
    "    keywords=set(keyword_json.keys())\n",
    "    for keyword in keywords:\n",
    "        attr_patterns.add(r'\\b%s\\b' % keyword)\n",
    "    patterns[attr]=attr_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attribute value extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Proximity based extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attr_extractor_proximity(text, people_spans, coref_spans, sentence_offsets, patterns, a_dict=None):\n",
    "    \"\"\"Generic extractor that operates based on patterns.\"\"\"\n",
    "    \n",
    "    extracted_pairs=defaultdict(list)\n",
    "    for pattern in patterns:\n",
    "        r=re.compile(pattern, re.IGNORECASE)\n",
    "        values=r.finditer(text)\n",
    "        for val_found in values:\n",
    "            span=val_found.span()\n",
    "            value=val_found.group()\n",
    "            value=value.replace('-year-old', '').replace(',', '').strip()\n",
    "            if a_dict and value in a_dict:\n",
    "                value=a_dict[value]\n",
    "            person, distance=utils.find_closest_person(span, \n",
    "                                                       people_spans, \n",
    "                                                       coref_spans, \n",
    "                                                       sentence_offsets,\n",
    "                                                       min_dist=1000)\n",
    "            if person:\n",
    "                extracted_pairs[person].append(tuple([distance, value]))\n",
    "    clean_pairs=utils.get_closest_value_per_person(extracted_pairs)\n",
    "    return clean_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Coreference based extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attr_extractor_coref(clusters, names, values_json, debug=False): #, text, people_spans):\n",
    "    \n",
    "    person_data=defaultdict(list)\n",
    "\n",
    "    if not clusters:\n",
    "        return person_data\n",
    "    \n",
    "    for c in clusters:\n",
    "        mentions=utils.stringify_cluster_mentions(c.mentions)\n",
    "        for person_name in names:\n",
    "            if utils.lookup_person_in_list(person_name, mentions):\n",
    "                for m in c.mentions:\n",
    "                    for txt in [m.text, m.lemma_]:\n",
    "                        if txt.lower() in values_json.keys():\n",
    "                            person_data[person_name].append(values_json[txt.lower()])\n",
    "                            \n",
    "    clean_data={}\n",
    "    for person_name, gs in person_data.items():\n",
    "        c=Counter(gs).most_common(1)[0][0]\n",
    "        clean_data[person_name]=c\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Run extraction of all properties for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_properties(names, full_text, nlp):\n",
    "    \n",
    "    doc = nlp(full_text)\n",
    "    # run coreference\n",
    "    people_spans, coref_spans, clusters=utils.get_coref_spans(names, full_text, doc)\n",
    "    \n",
    "    sentence_offsets=utils.get_sentence_offsets(doc)\n",
    "    \n",
    "    all_extracted={}\n",
    "    for attribute in pattern_attrs:\n",
    "        all_extracted[attribute]=attr_extractor_proximity(full_text, \n",
    "                                                           people_spans, \n",
    "                                                           coref_spans, \n",
    "                                                           sentence_offsets, \n",
    "                                                           a_dict=pattern_data[attribute],\n",
    "                                                           patterns=patterns[attribute])\n",
    "    \n",
    "    all_extracted['gender']=attr_extractor_coref(clusters, names, genders)\n",
    "    \n",
    "    #combine extractors\n",
    "    combined=utils.singularize_data(all_extracted)\n",
    "            \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Peter Boer': {'age': '26',\n",
       "  'ethnicity': 'white/caucascian',\n",
       "  'religion': 'christian',\n",
       "  'causeofdeath': 'intentional',\n",
       "  'residence': 'texas'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test stuff\n",
    "\n",
    "txt='Hello Peter Boer, 26 was shot in church from Houston. The white police guy failed.'\n",
    "names=['Peter Boer']\n",
    "extract_properties(names, txt, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run GVDB extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_gvdb_data(the_file, limit=200):\n",
    "\n",
    "    all_gold_rows=[]\n",
    "    all_sys_rows=[]\n",
    "\n",
    "    with open(the_file, 'r') as csvfile:\n",
    "        rdr = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
    "        header=next(rdr)\n",
    "        \n",
    "        for c, row in enumerate(rdr):\n",
    "            if c==limit: break\n",
    "\n",
    "            full_text=row[2]\n",
    "            data=json.loads(row[3])\n",
    "            part_info=utils.get_participant_info(data)\n",
    "\n",
    "            names=set(part_info.keys())\n",
    "            if not len(names): continue\n",
    "\n",
    "            system_data = extract_properties(names, full_text, nlp)\n",
    "            all_sys_rows.append(system_data)\n",
    "            all_gold_rows.append(part_info)\n",
    "            \n",
    "            c+=1\n",
    "    return all_sys_rows, all_gold_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_sys_rows, all_gold_rows = process_gvdb_data(gvdb_articles_file, limit=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run SE extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_se_data(annotation_file, text_file, limit=200):\n",
    "\n",
    "    annotations=utils.load_pickle(annotation_file)\n",
    "    all_texts_json=utils.load_pickle(text_file)\n",
    "    \n",
    "    all_gold_rows=[]\n",
    "    all_sys_rows=[]\n",
    "        \n",
    "    storable = {}\n",
    "        \n",
    "    cnt=0\n",
    "\n",
    "    for doc_id, part_data in annotations.items():\n",
    "        storable[doc_id]={}\n",
    "        \n",
    "        names=utils.get_names(part_data)\n",
    "        if doc_id not in all_texts_json:\n",
    "            text=utils.load_text_from_json('%s/%s.json' % (se_all_documents_path, doc_id))\n",
    "        else:\n",
    "            text_json=all_texts_json[doc_id]\n",
    "            conll_data=text_json['content']\n",
    "            text=utils.conll_to_text(conll_data)\n",
    "        \n",
    "        properties=extract_properties(names, text, nlp)\n",
    "        \n",
    "        all_sys_rows.append(properties)\n",
    "             \n",
    "        for part_id, a_part_info in part_data.items():\n",
    "            if 'Name' not in a_part_info.keys() or not a_part_info['Name']: continue\n",
    "\n",
    "            name=a_part_info['Name'].strip()\n",
    "\n",
    "            these_properties={}\n",
    "            if name in properties.keys():\n",
    "                for k,v in properties[name].items():\n",
    "                    these_properties[map_properties[k]]=v\n",
    "\n",
    "            these_properties['Name']=name\n",
    "            storable[doc_id][part_id]=these_properties\n",
    "\n",
    "        part_info=utils.transform_part_info(part_data)\n",
    "        all_gold_rows.append(part_info)\n",
    "        \n",
    "        cnt+=1\n",
    "        if cnt==limit: break\n",
    "        \n",
    "    return all_sys_rows, all_gold_rows, storable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "african american ad74870db8920e6986b76133117e7a82\n",
      "african american 65d7c38113863b1bc6cee2e0fa38dc6c\n",
      "hispanic/latin a51f975867abd1f18661d858fabf9c3f\n",
      "white/caucascian 4026e33b1c2033fb5433740895842102\n",
      "african american 6bb0bddf01c6230df34d1ce9cd7869a6\n",
      "hispanic/latin 8695354c9046509b209a5aa92f48c199\n",
      "african american 6bb255cee652c51bb6d34444b1a76705\n",
      "african american e95dc411de0c45356810076ce4e50890\n",
      "african american 42ab147825ee27d8a354517061216a91\n",
      "african american 2a7e35c5897effac0205abe1f82af96d\n"
     ]
    }
   ],
   "source": [
    "se_system, se_gold, data_to_store = process_se_data(se_annotation_file, \n",
    "                                                    se_partial_text_file, \n",
    "                                                    limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'residence': 404,\n",
       "             'age': 740,\n",
       "             'gender': 757,\n",
       "             'educationlevel': 117,\n",
       "             'causeofdeath': 489,\n",
       "             'deathplace': 566,\n",
       "             'deathdate': 566,\n",
       "             'birthplace': 6,\n",
       "             'pastconviction': 32,\n",
       "             'religion': 17,\n",
       "             'medicalcondition': 8,\n",
       "             'ethnicity': 10})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.count_per_attribute(se_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Benchmark extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Benchmark GVDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gvdb_attributes=['age', 'race', 'gender']\n",
    "# utils.benchmark_extractors(all_sys_rows, all_gold_rows, gvdb_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Benchmark on SemEval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'age' in se_attributes:\n",
    "    se_system=utils.map_age_to_group(se_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 sys african american\n",
      "129 sys african american\n",
      "135 gold african american\n",
      "135 gold african american\n",
      "176 gold hispanic/latin\n",
      "239 gold white/caucascian\n",
      "270 gold african american\n",
      "294 gold hispanic/latin\n",
      "305 gold african american\n",
      "335 gold african american\n",
      "364 gold african american\n",
      "404 gold african american\n",
      "421 sys african american\n",
      "defaultdict(<class 'int'>, {'age': 192, 'educationlevel': 23, 'residence': 96, 'gender': 103, 'causeofdeath': 84, 'religion': 2, 'pastconviction': 8}) defaultdict(<class 'int'>, {'pastconviction': 76, 'causeofdeath': 139, 'educationlevel': 14, 'residence': 154, 'age': 14, 'birthplace': 1, 'gender': 14, 'ethnicity': 3, 'religion': 2}) defaultdict(<class 'int'>, {'age': 548, 'residence': 308, 'gender': 654, 'causeofdeath': 405, 'educationlevel': 94, 'birthplace': 6, 'religion': 15, 'pastconviction': 24, 'ethnicity': 10})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'age': 0.9320388349514563,\n",
       "  'ethnicity': 0.0,\n",
       "  'causeofdeath': 0.37668161434977576,\n",
       "  'religion': 0.5,\n",
       "  'educationlevel': 0.6216216216216216,\n",
       "  'pastconviction': 0.09523809523809523,\n",
       "  'residence': 0.384,\n",
       "  'birthplace': 0.0,\n",
       "  'gender': 0.8803418803418803},\n",
       " {'age': 0.2594594594594595,\n",
       "  'ethnicity': 0.0,\n",
       "  'causeofdeath': 0.17177914110429449,\n",
       "  'religion': 0.11764705882352941,\n",
       "  'educationlevel': 0.19658119658119658,\n",
       "  'pastconviction': 0.25,\n",
       "  'residence': 0.2376237623762376,\n",
       "  'birthplace': 0.0,\n",
       "  'gender': 0.13606340819022458},\n",
       " {'age': 0.4059196617336152,\n",
       "  'ethnicity': 0.0,\n",
       "  'causeofdeath': 0.23595505617977527,\n",
       "  'religion': 0.19047619047619047,\n",
       "  'educationlevel': 0.29870129870129875,\n",
       "  'pastconviction': 0.13793103448275862,\n",
       "  'residence': 0.29357798165137616,\n",
       "  'birthplace': 0.0,\n",
       "  'gender': 0.2356979405034325})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.benchmark_extractors(se_system, se_gold, se_attributes, debug='ethnicity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare SE output to be run by the baselines\n",
    "\n",
    "Desired format: pickle\n",
    "\n",
    "```\n",
    "{\n",
    "    doc_id:\n",
    "    {\n",
    "        part_id:\n",
    "        {\n",
    "            prop: value, \n",
    "            prop2: value2,\n",
    "            ...\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Process the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.dump_pickle(data_to_store, se_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Process the altered versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/input/partial/annotation/participants_samefirstname.p ../data/input/partial/annotation/participants_input.p\n",
      "not found number: 9\n",
      "extracted_data/partial/participants_samefirstname.p\n",
      "../data/input/partial/annotation/participants_samename.p ../data/input/partial/annotation/participants_input.p\n",
      "not found number: 9\n",
      "extracted_data/partial/participants_samename.p\n",
      "../data/input/partial/annotation/participants_samelastname.p ../data/input/partial/annotation/participants_input.p\n",
      "not found number: 9\n",
      "extracted_data/partial/participants_samelastname.p\n"
     ]
    }
   ],
   "source": [
    "for f in glob.glob('%s/*.p' % se_annotation_dir):\n",
    "    if f.strip()!=se_annotation_file:\n",
    "        print(f, se_annotation_file)\n",
    "        altered_data=utils.load_pickle(f)\n",
    "        \n",
    "        not_found=0\n",
    "        new_data=defaultdict(dict)\n",
    "        for doc_id, doc_data in altered_data.items():\n",
    "            doc_property_data=data_to_store[doc_id]\n",
    "            \n",
    "            for part_id, part_data in doc_data.items():\n",
    "                if part_id not in doc_property_data.keys():\n",
    "                    #print('NOT FOUND IN THE EXTRACTED DATA. document:', doc_id, '; participant:', part_id)\n",
    "                    not_found+=1\n",
    "                    continue\n",
    "                    \n",
    "                new_part_data={}\n",
    "                if 'Name' in part_data:\n",
    "                    new_part_data['Name'] = part_data['Name']\n",
    "                for k, v in doc_property_data[part_id].items():\n",
    "                    if k!='Name':\n",
    "                        new_part_data[k]=v\n",
    "                new_data[doc_id][part_id]=new_part_data\n",
    "\n",
    "        print('not found number:', not_found)\n",
    "        new_file_path='%s/%s' % (se_output_dir, f.split('/')[-1])\n",
    "        print(new_file_path)\n",
    "        \n",
    "        utils.dump_pickle(new_data, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
