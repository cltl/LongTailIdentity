{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_dir='profiler_output'\n",
    "givens_path='%s/given.pkl' % profiled_dir\n",
    "predicted_path='%s/predicted.pkl' % profiled_dir\n",
    "\n",
    "input_dir='../data/input/partial/annotation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties=['native language' , 'ethnic group', 'cause of death', 'sex or gender', 'religion', 'member of political party', 'occupation', 'age group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse profiling result\n",
    "\n",
    "#### 1a. Load the profiling predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_profiler_predictions(givens_file, predicted_file, properties):\n",
    "    with open(givens_file, 'rb',) as f:\n",
    "        givens=pickle.load(f, encoding='latin1')\n",
    "    with(open(predicted_file, 'rb')) as f:\n",
    "        predicted=pickle.load(f, encoding='latin1')\n",
    "        \n",
    "    data={}\n",
    "    for index, givens_row in enumerate(givens):\n",
    "        ready_key=[]\n",
    "        ready_value={}\n",
    "        for p in properties:\n",
    "            if p in givens_row.keys():\n",
    "                ready_key.append(givens_row[p])\n",
    "            else:\n",
    "                ready_key.append('')\n",
    "                ready_value[p]=predicted[p][index]\n",
    "        data[tuple(ready_key)]=ready_value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_data=load_profiler_predictions(givens_path, predicted_path, properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Merge with existing data to prepare for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_tuple(properties, part_data):\n",
    "    part_for_profiler=['']*len(properties)\n",
    "    if 'Ethnicity' in part_data.keys():\n",
    "        v=part_data['Ethnicity'].strip()\n",
    "        if v=='African American':\n",
    "            v='African American/Black'\n",
    "        if v=='White/Caucascian':\n",
    "            v='White/Caucasian'\n",
    "        part_for_profiler[1]=v\n",
    "    if 'CauseOfDeath' in part_data.keys():\n",
    "        part_for_profiler[2]=part_data['CauseOfDeath'].strip()\n",
    "    if 'Gender' in part_data.keys():\n",
    "        part_for_profiler[3]=part_data['Gender'].strip().lower()\n",
    "    if 'Religion' in part_data.keys():\n",
    "        part_for_profiler[4]=part_data['Religion'].strip()\n",
    "    if 'Occupation' in part_data.keys():\n",
    "        part_for_profiler[6]=part_data['Occupation'].strip()\n",
    "    if 'Age' in part_data.keys():\n",
    "        part_for_profiler[7]=part_data['Age'].strip().lower()\n",
    "    tuple_input=tuple(part_for_profiler)\n",
    "\n",
    "    return tuple_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_profiler_data(input_file, properties, profiled_data):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        participants=pickle.load(f)\n",
    "\n",
    "    parts_per_name=defaultdict(dict)\n",
    "    \n",
    "    for doc_id, doc_data in participants.items():\n",
    "        for part_id, part_data in doc_data.items():\n",
    "            name=''\n",
    "            if 'Name' in part_data.keys():\n",
    "                name=part_data['Name']\n",
    "\n",
    "            tuple_input=get_property_tuple(properties, part_data)\n",
    "            values=profiled_data[tuple_input]\n",
    "            \n",
    "            for index, t in enumerate(tuple_input):\n",
    "                if t!='':\n",
    "                    values[properties[index]]=[tuple([t, '1.0'])]\n",
    "            \n",
    "            parts_per_name[name][part_id]=values\n",
    "                \n",
    "    return parts_per_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute similarity with JS entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_divergence(c1, c2):\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_js_divergences(c1, c2, properties):\n",
    "    divs=[]\n",
    "    for p in properties:\n",
    "        div=js_divergence(c1[p], c2[p])\n",
    "        divs.append(div)\n",
    "    return divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(candidates, properties, threshold=0.45):\n",
    "    clusters_json={}\n",
    "    \n",
    "    pairs=list(itertools.combinations(candidates.keys(), 2))\n",
    "    \n",
    "    for p1,p2 in pairs:\n",
    "        c1=candidates[p1]\n",
    "        c2=candidates[p2]\n",
    "        divs=compute_js_divergences(c1, c2, properties)\n",
    "        \n",
    "        avg=sum(divs)/len(divs)\n",
    "        if avg<threshold:\n",
    "            print(p1, p2, avg, True)\n",
    "        else:\n",
    "            print(p1, p2, avg, False)\n",
    "    \n",
    "    return clusters_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(data, properties):\n",
    "    for name, name_candidates in data.items():\n",
    "        clusters=compute_clusters(name_candidates, properties)\n",
    "        print(name, clusters)\n",
    "        input('continue?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nathaniel Jones {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " E.C. Robinson {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kendall Reed {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Will Harris {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Steven Coleman {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197305597b1d8d10976862afb0febb8e d8dcbfa71b9b5cf7e3bf283772df7411 0.5 False\n",
      " Christopher Roupe {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22469ebbde972ab665bec2328e3e8281 695f99ae21f8641cc9a3b7ff648a1473 0.5 False\n",
      "22469ebbde972ab665bec2328e3e8281 b0a0251c68d927d4f5ae0ae5d2473018 0.5 False\n",
      "695f99ae21f8641cc9a3b7ff648a1473 b0a0251c68d927d4f5ae0ae5d2473018 0.5 False\n",
      " Brady Osborne {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c33d0ab46bf6dbc51b78d6c552d196d8 67290b5ca2d1c930c89a270061cc1061 0.5 False\n",
      "c33d0ab46bf6dbc51b78d6c552d196d8 ef8ea80ae83aa93f6b0241a4ec6775d6 0.5 False\n",
      "67290b5ca2d1c930c89a270061cc1061 ef8ea80ae83aa93f6b0241a4ec6775d6 0.5 False\n",
      " Matt Anderson {}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "continue? \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c34db0836a8c4a062fe76b45da2939fb 161753d585e7a28ab67bde6e67ccb699 0.5 False\n",
      " Braison Howard {}\n"
     ]
    }
   ],
   "source": [
    "test=perform_clustering(data, properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in glob.glob('%s/*.p' % input_dir):\n",
    "    data=prepare_profiler_data(f, properties, profiled_data)\n",
    "    clusters=perform_clustering(data)\n",
    "    store_clusters(clusters)\n",
    "    print(f)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
