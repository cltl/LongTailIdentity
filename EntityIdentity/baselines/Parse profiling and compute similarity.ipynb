{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_eps=0.2 # 0.05 or 0.1\n",
    "which_extractor='auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_dir='profiler_output'\n",
    "givens_path='%s/%s_given.pkl' % (profiled_dir, which_extractor)\n",
    "predicted_path='%s/%s_predicted.pkl' % (profiled_dir, which_extractor)\n",
    "\n",
    "if which_extractor=='gold':\n",
    "    input_dir='../data/input/partial/annotation'\n",
    "    output_dir='../data/system/gold_profiling/partial'\n",
    "else:\n",
    "    input_dir='extracted_data/partial'\n",
    "    output_dir='../data/system/auto_profiling/partial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties=['native language' , 'ethnic group', 'cause of death', 'sex or gender', 'religion', 'member of political party', 'occupation', 'age group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mapping_file='../resources/gv_mappings.json'\n",
    "with open(value_mapping_file, 'r') as f:\n",
    "    wikidata_to_labels=json.load(f)\n",
    "\n",
    "prop_vals={}\n",
    "for prop, vals in wikidata_to_labels.items():\n",
    "    prop_vals[prop]=list(set(vals.values())) + ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_vals['age group']=['child 0-11', 'teen 12-17', 'adult 18-64', 'senior 65+', '']\n",
    "prop_vals['ethnic group'].append('Hispanic/Latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse profiling result\n",
    "\n",
    "#### 1a. Load the profiling predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_profiler_predictions(givens_file, predicted_file, properties):\n",
    "    with open(givens_file, 'rb',) as f:\n",
    "        givens=pickle.load(f, encoding='latin1')\n",
    "    with(open(predicted_file, 'rb')) as f:\n",
    "        predicted=pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    data={}\n",
    "    for index, givens_row in enumerate(givens):\n",
    "        ready_key=[]\n",
    "        ready_value={}\n",
    "        for p in properties:\n",
    "            if p in givens_row.keys():\n",
    "                ready_key.append(givens_row[p])\n",
    "            else:\n",
    "                ready_key.append('')\n",
    "                ready_value[p]=predicted[p][index]\n",
    "        data[tuple(ready_key)]=ready_value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_data=load_profiler_predictions(givens_path, predicted_path, properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('', '', '', '', '', '', '', ''), ('', '', '', '', '', '', '', 'teen 12-17'), ('', '', 'Intentional', 'male', '', '', '', ''), ('', '', 'Accidental', '', '', '', '', 'teen 12-17'), ('', '', 'Accidental', 'male', '', '', '', 'teen 12-17'), ('', '', 'Accidental', '', '', '', '', 'child 0-11'), ('', '', 'Intentional', '', '', '', '', ''), ('', '', '', '', '', '', '', 'adult 18-64'), ('', '', '', 'female', 'Christianity', '', '', 'child 0-11'), ('', '', 'Intentional', 'male', '', '', '', 'teen 12-17'), ('', '', 'Accidental', 'male', '', '', '', ''), ('', '', '', 'male', '', '', '', ''), ('', '', 'Accidental', '', '', '', '', 'adult 18-64'), ('', '', 'Intentional', '', '', '', '', 'teen 12-17'), ('', '', 'Intentional', 'female', '', '', '', ''), ('', '', '', '', '', '', '', 'child 0-11'), ('', '', '', 'female', '', '', '', ''), ('', '', 'Suicide', '', '', '', '', ''), ('', '', 'Suicide', 'male', '', '', '', 'adult 18-64'), ('', '', 'Suicide', 'male', '', '', '', ''), ('', '', 'Intentional', '', '', '', '', 'child 0-11'), ('', '', 'Intentional', '', '', '', '', 'adult 18-64'), ('', '', '', 'female', '', '', '', 'child 0-11'), ('', '', 'Accidental', '', '', '', '', ''), ('', '', '', '', '', '', 'sports player', ''), ('', '', 'Suicide', '', '', '', '', 'adult 18-64'), ('', '', '', 'male', '', '', '', 'adult 18-64'), ('', '', 'Accidental', 'female', '', '', '', ''), ('', '', 'Intentional', 'male', '', '', '', 'adult 18-64'), ('', '', 'Intentional', 'female', '', '', '', 'adult 18-64'), ('', 'African American/Black', '', '', '', '', '', ''), ('', '', '', 'female', '', '', '', 'adult 18-64'), ('', '', '', '', 'Christianity', '', '', ''), ('', '', 'Intentional', '', '', '', 'sports player', 'adult 18-64'), ('', '', 'Suicide', 'male', '', '', '', 'child 0-11'), ('', '', 'Suicide', 'female', '', '', '', ''), ('', '', 'Intentional', '', '', '', 'sports player', 'teen 12-17'), ('', '', '', 'male', '', '', '', 'child 0-11'), ('', '', '', '', '', '', '', 'senior 65+'), ('', '', 'Intentional', 'male', 'Christianity', '', '', ''), ('', '', 'Accidental', 'male', '', '', 'sports player', ''), ('', 'African American/Black', 'Intentional', 'male', '', '', '', ''), ('', '', '', 'male', '', '', '', 'teen 12-17')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiled_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Merge with existing data to prepare for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_tuple(properties, part_data):\n",
    "    part_for_profiler=['']*len(properties)\n",
    "    if 'Ethnicity' in part_data.keys():\n",
    "        v=part_data['Ethnicity'].strip()\n",
    "        if v.lower()=='african american':\n",
    "            v='African American/Black'\n",
    "        if v.lower()=='white/caucascian':\n",
    "            v='White/Caucasian'\n",
    "        part_for_profiler[1]=v\n",
    "    if 'CauseOfDeath' in part_data.keys():\n",
    "        part_for_profiler[2]=part_data['CauseOfDeath'].strip()\n",
    "    if 'Gender' in part_data.keys():\n",
    "        part_for_profiler[3]=part_data['Gender'].strip().lower()\n",
    "    if 'Religion' in part_data.keys():\n",
    "        v=part_data['Religion'].strip()\n",
    "        if v.lower()=='christian':\n",
    "            v='Christianity'\n",
    "        part_for_profiler[4]=v\n",
    "    if 'Occupation' in part_data.keys():\n",
    "        part_for_profiler[6]=part_data['Occupation'].strip()\n",
    "    if 'Age' in part_data.keys():\n",
    "        part_for_profiler[7]=part_data['Age'].strip().lower()\n",
    "    \n",
    "    if which_extractor=='auto':\n",
    "        norm_input=normalize_values(part_for_profiler)\n",
    "        tuple_input=tuple(norm_input)\n",
    "    else:\n",
    "        tuple_input=tuple(part_for_profiler)\n",
    "\n",
    "    \n",
    "    return tuple_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_age(a):\n",
    "    if a<12:\n",
    "        return 'child 0-11'\n",
    "    elif a<18:\n",
    "        return 'teen 12-17'\n",
    "    elif a<65:\n",
    "        return 'adult 18-64'\n",
    "    else:\n",
    "        return 'senior 65+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_occupations(o):\n",
    "    mappings={'basketball': 'sports player', 'rugby': 'sports player', 'football player': 'sports player', 'sports': 'sports player'}\n",
    "    if o in mappings.keys():\n",
    "        return mappings[o]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(row, debug=False):\n",
    "    \n",
    "    #debug=True\n",
    "\n",
    "    new_row=row\n",
    "\n",
    "    cause_of_death=row[2]\n",
    "    if cause_of_death:\n",
    "        new_row[2]=cause_of_death.capitalize()\n",
    "        if new_row[2]=='Negligent':\n",
    "            new_row[2]='Accidental'\n",
    "        elif new_row[2] not in {'Intentional', 'Accidental', 'Suicide'}:\n",
    "            new_row[2]=''\n",
    "\n",
    "\n",
    "    age=row[7]\n",
    "    if age:\n",
    "        age_group=group_age(int(age))\n",
    "        new_row[7]=age_group\n",
    "\n",
    "    occupation=row[6]\n",
    "    if occupation:\n",
    "        new_row[6]=map_occupations(occupation)\n",
    "\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_profiler_data(input_file, properties, profiled_data, debug=False):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        participants=pickle.load(f)\n",
    "\n",
    "    parts_per_name=defaultdict(dict)\n",
    "    \n",
    "    values_per_name=defaultdict(dict)\n",
    "    \n",
    "    for doc_id, doc_data in participants.items():\n",
    "        for part_id, part_data in doc_data.items():\n",
    "            name=''\n",
    "            if 'Name' in part_data.keys():\n",
    "                name=part_data['Name'].strip()\n",
    "\n",
    "            tuple_input=get_property_tuple(properties, part_data)\n",
    "            values=profiled_data[tuple_input]\n",
    "            \n",
    "            for index, t in enumerate(tuple_input):\n",
    "                if t!='':\n",
    "                    values[properties[index]]=[tuple([t, '1.0'])]\n",
    "            \n",
    "            parts_per_name[name][part_id]=values\n",
    "            \n",
    "            #parts_per_name[name][part_id]=tuple_input\n",
    "            \n",
    "    #print(parts_per_name['Hayden Mayes'])\n",
    "    #if debug:\n",
    "#        print(values_per_name)\n",
    "        #a_name='Hayden Mayes'\n",
    "        #print(values_per_name[a_name])\n",
    "        #print(parts_per_name[a_name])\n",
    "        #sys.exit()\n",
    "    \n",
    "    return parts_per_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute similarity with JS entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js(p, q):\n",
    "    p = np.asarray(p).astype(np.float)\n",
    "    q = np.asarray(q).astype(np.float)\n",
    "   # normalize\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = (p + q) / 2\n",
    "    return (entropy(p, m) + entropy(q, m)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(v1, v2, domain):\n",
    "    l=len(domain)\n",
    "    mapped1=[0]*l\n",
    "    mapped2=[0]*l\n",
    "    \n",
    "    for k,v in v1:\n",
    "        index=domain.index(k)\n",
    "        mapped1[index]=v\n",
    "    for k,v in v2:\n",
    "        index=domain.index(k)\n",
    "        mapped2[index]=v\n",
    "    return mapped1, mapped2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_js_divergences(c1, c2, properties):\n",
    "    divs=[]\n",
    "    for p in properties:\n",
    "        mapped1, mapped2 = map_values(c1[p], c2[p], prop_vals[p])\n",
    "        div=js(mapped1, mapped2)\n",
    "        divs.append(div)\n",
    "    return divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "def cluster_matrix_with_features(matrix, algorithm='ward', max_d=0.3, criterion='distance'):\n",
    "    merges = linkage(matrix, 'ward')\n",
    "    clusters = fcluster(merges, max_d, criterion=criterion)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_matrix(distances, eps=0.1, min_samples=1):\n",
    "    labels=DBSCAN(min_samples=min_samples, eps=eps, metric='precomputed').fit_predict(distances)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "        \n",
    "    return list(labels), n_clusters, n_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(candidates, properties, start_id, eps):\n",
    "\n",
    "    # intitialize an empty matrix\n",
    "    num_cands=len(candidates.keys())\n",
    "    dist_matrix = np.zeros(shape=(num_cands, num_cands)) # Distances matrix\n",
    "    \n",
    "    # fill the matrix with similarity values\n",
    "    the_keys=list(candidates.keys())    \n",
    "    for index1, p1 in enumerate(the_keys):\n",
    "        for index2, p2 in enumerate(the_keys):\n",
    "            if index1<index2:\n",
    "                c1=candidates[p1]\n",
    "                c2=candidates[p2]\n",
    "                divs=compute_js_divergences(c1, c2, properties)\n",
    "                avg_div=sum(divs)/len(divs)\n",
    "        \n",
    "                dist_matrix[index1, index2]=avg_div\n",
    "                dist_matrix[index2, index1]=avg_div\n",
    "                \n",
    "    # run clustering\n",
    "    clusters, n_clusters, n_noise = cluster_matrix(dist_matrix, eps=eps)\n",
    "    clusters_json={}\n",
    "    for index, part_id in enumerate(the_keys):\n",
    "        cluster_id=start_id+int(clusters[index])\n",
    "        clusters_json[part_id]=cluster_id\n",
    "    \n",
    "    new_start_id=start_id+n_clusters\n",
    "    \n",
    "    return clusters_json, new_start_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(data, properties, eps):\n",
    "    \n",
    "    clusters={}\n",
    "    start_id=1\n",
    "    for name, name_candidates in data.items():\n",
    "        new_clusters, new_start_id=compute_clusters(name_candidates, properties, start_id, eps)\n",
    "        #if name=='Hayden Mayes':\n",
    "        #    print(new_clusters)\n",
    "        clusters.update(new_clusters)\n",
    "        start_id=new_start_id\n",
    "    print(start_id)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_clusters(clusters, output_file):\n",
    "    with open(output_file, 'w') as w:\n",
    "        json.dump(clusters, w)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_data/partial/participants_input.p\n",
      "465\n",
      "extracted_data/partial/participants_samefirstname.p\n",
      "333\n",
      "extracted_data/partial/participants_samename.p\n",
      "2\n",
      "extracted_data/partial/participants_samelastname.p\n",
      "385\n",
      "extracted_data/partial/extracted_data.p\n",
      "465\n"
     ]
    }
   ],
   "source": [
    "for f in glob.glob('%s/*.p' % input_dir):\n",
    "    print(f)\n",
    "    output_file='%s/%s.json' % (output_dir, (f.split('/')[-1]).split('.')[0])\n",
    "    data=prepare_profiler_data(f, properties, profiled_data, debug=True)\n",
    "    clusters=perform_clustering(data, properties, clustering_eps)\n",
    "    store_clusters(clusters, output_file)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
