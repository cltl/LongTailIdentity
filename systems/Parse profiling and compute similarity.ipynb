{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster_utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_extractor='gold' # gold or auto\n",
    "which_partition='partial' # partial or full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_dir='profiler_output'\n",
    "givens_path='%s/%s_%s_given.pkl' % (profiled_dir, which_extractor, which_partition)\n",
    "predicted_path='%s/%s_%s_predicted.pkl' % (profiled_dir, which_extractor, which_partition)\n",
    "\n",
    "if which_extractor=='gold':\n",
    "    input_dir='../data/input/%s/annotation' % which_partition\n",
    "    output_dir='../data/system/gold_profiling/%s' % which_partition\n",
    "else:\n",
    "    input_dir='extracted_data/%s' % which_partition\n",
    "    output_dir='../data/system/auto_profiling/%s' % which_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties=['native language' , 'ethnic group', 'cause of death', 'sex or gender', 'religion', 'member of political party', 'occupation', 'age group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mapping_file='../resources/gv_mappings.json'\n",
    "with open(value_mapping_file, 'r') as f:\n",
    "    wikidata_to_labels=json.load(f)\n",
    "\n",
    "prop_vals={}\n",
    "for prop, vals in wikidata_to_labels.items():\n",
    "    prop_vals[prop]=list(set(vals.values())) + ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_vals['age group']=['child 0-11', 'teen 12-17', 'adult 18-64', 'senior 65+', '']\n",
    "prop_vals['ethnic group'].append('Hispanic/Latin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse profiling result\n",
    "\n",
    "#### 1a. Load the profiling predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_profiler_predictions(givens_file, predicted_file, properties):\n",
    "    with open(givens_file, 'rb',) as f:\n",
    "        givens=pickle.load(f, encoding='latin1')\n",
    "    with(open(predicted_file, 'rb')) as f:\n",
    "        predicted=pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    data={}\n",
    "    for index, givens_row in enumerate(givens):\n",
    "        ready_key=[]\n",
    "        ready_value={}\n",
    "        for p in properties:\n",
    "            if p in givens_row.keys():\n",
    "                ready_key.append(givens_row[p])\n",
    "            else:\n",
    "                ready_key.append('')\n",
    "                ready_value[p]=predicted[p][index]\n",
    "        data[tuple(ready_key)]=ready_value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_data=load_profiler_predictions(givens_path, predicted_path, properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('', '', '', 'male', '', '', '', 'senior 65+'), ('', '', '', 'male', '', '', '', 'adult 18-64'), ('', '', 'Intentional', 'male', '', '', '', 'teen 12-17'), ('', '', '', 'male', '', '', '', 'teen 12-17'), ('', '', 'Accidental', 'male', '', '', '', 'teen 12-17'), ('', '', 'Accidental', 'male', '', '', '', 'child 0-11'), ('', '', 'Suicide', 'male', 'Christianity', '', '', 'adult 18-64'), ('', '', 'Intentional', 'female', 'Christianity', '', '', 'child 0-11'), ('', '', 'Intentional', 'female', 'Christianity', '', '', 'teen 12-17'), ('', '', 'Intentional', 'female', 'Christianity', '', '', 'adult 18-64'), ('', '', '', 'male', 'Christianity', '', '', 'child 0-11'), ('', '', '', 'male', '', '', '', 'child 0-11'), ('', '', 'Accidental', 'male', 'Christianity', '', '', 'teen 12-17'), ('', '', '', 'female', '', '', '', 'child 0-11'), ('', '', 'Accidental', 'female', '', '', '', 'teen 12-17'), ('', '', '', 'female', '', '', '', 'adult 18-64'), ('', '', 'Suicide', 'female', '', '', '', 'adult 18-64'), ('', '', 'Intentional', 'male', '', '', '', 'child 0-11'), ('', '', 'Suicide', 'male', '', '', '', 'adult 18-64'), ('', '', 'Intentional', 'female', '', '', '', 'adult 18-64'), ('', '', 'Intentional', 'female', '', '', '', 'child 0-11'), ('', '', '', 'female', '', '', '', 'teen 12-17'), ('', '', 'Intentional', 'male', '', '', '', 'adult 18-64'), ('', '', 'Accidental', 'male', '', '', '', 'adult 18-64'), ('', '', 'Accidental', 'female', '', '', '', 'child 0-11'), ('', '', 'Suicide', 'male', '', '', '', 'teen 12-17'), ('', '', 'Intentional', 'male', '', '', '', ''), ('', '', '', 'male', '', '', '', ''), ('', 'African American/Black', 'Accidental', 'male', '', '', '', 'teen 12-17'), ('', 'African American/Black', '', 'male', '', '', '', 'teen 12-17'), ('', '', 'Suicide', 'male', '', '', '', 'child 0-11'), ('', '', 'Accidental', 'female', '', '', '', 'adult 18-64'), ('', 'Hispanic/Latin', '', 'male', '', '', '', 'adult 18-64'), ('', '', '', '', '', '', '', 'adult 18-64'), ('', '', '', '', '', '', '', ''), ('', 'White/Caucasian', '', 'male', '', '', '', 'adult 18-64'), ('', '', 'Intentional', 'female', '', '', '', ''), ('', '', 'Intentional', 'female', '', '', '', 'senior 65+'), ('', '', 'Intentional', 'female', '', '', '', 'teen 12-17'), ('', 'African American/Black', '', 'male', '', '', '', 'adult 18-64'), ('', '', 'Accidental', 'male', 'Christianity', '', '', 'adult 18-64'), ('', '', '', 'female', '', '', '', ''), ('', '', 'Intentional', 'male', '', '', '', 'senior 65+'), ('', 'Hispanic/Latin', '', 'male', '', '', '', 'teen 12-17'), ('', 'African American/Black', '', 'male', '', '', '', 'child 0-11'), ('', '', '', 'male', 'Christianity', '', '', 'teen 12-17'), ('', 'African American/Black', 'Intentional', 'male', '', '', '', 'teen 12-17'), ('', '', 'Accidental', 'male', '', '', '', ''), ('', '', '', '', '', '', '', 'child 0-11')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiled_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Merge with existing data to prepare for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_tuple(properties, part_data):\n",
    "    part_for_profiler=['']*len(properties)\n",
    "    if 'Ethnicity' in part_data.keys():\n",
    "        v=part_data['Ethnicity'].strip()\n",
    "        if v.lower()=='african american':\n",
    "            v='African American/Black'\n",
    "        if v.lower()=='white/caucascian' or v.lower()=='white':\n",
    "            v='White/Caucasian'\n",
    "        if v.lower()=='hispanic/latin':\n",
    "            v='Hispanic/Latin'\n",
    "        part_for_profiler[1]=v\n",
    "    if 'CauseOfDeath' in part_data.keys():\n",
    "        part_for_profiler[2]=part_data['CauseOfDeath'].strip()\n",
    "    if 'Gender' in part_data.keys():\n",
    "        part_for_profiler[3]=part_data['Gender'].strip().lower()\n",
    "    if 'Religion' in part_data.keys():\n",
    "        v=part_data['Religion'].strip()\n",
    "        if v.lower()=='christian':\n",
    "            v='Christianity'\n",
    "        part_for_profiler[4]=v\n",
    "    if 'Occupation' in part_data.keys():\n",
    "        part_for_profiler[6]=part_data['Occupation'].strip()\n",
    "    if 'Age' in part_data.keys():\n",
    "        part_for_profiler[7]=part_data['Age'].strip().lower()\n",
    "    if which_extractor=='gold' and which_partition=='partial':\n",
    "        tuple_input=tuple(part_for_profiler)\n",
    "    else:\n",
    "        norm_input=normalize_values(part_for_profiler)\n",
    "        tuple_input=tuple(norm_input)\n",
    "    \n",
    "    return tuple_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_age(a):\n",
    "    if a<12:\n",
    "        return 'child 0-11'\n",
    "    elif a<18:\n",
    "        return 'teen 12-17'\n",
    "    elif a<65:\n",
    "        return 'adult 18-64'\n",
    "    else:\n",
    "        return 'senior 65+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_occupations(o):\n",
    "    mappings={'basketball': 'sports player', 'rugby': 'sports player', 'football player': 'sports player', 'sports': 'sports player'}\n",
    "    if o in mappings.keys():\n",
    "        return mappings[o]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(row, debug=False):\n",
    "    \n",
    "    #debug=True\n",
    "\n",
    "    new_row=row\n",
    "\n",
    "    cause_of_death=row[2]\n",
    "    if cause_of_death:\n",
    "        new_row[2]=cause_of_death.capitalize()\n",
    "        if new_row[2]=='Negligent':\n",
    "            new_row[2]='Accidental'\n",
    "        elif new_row[2] not in {'Intentional', 'Accidental', 'Suicide'}:\n",
    "            new_row[2]=''\n",
    "\n",
    "\n",
    "    age=row[7]\n",
    "    if age:\n",
    "        age_group=group_age(int(age))\n",
    "        new_row[7]=age_group\n",
    "\n",
    "    occupation=row[6]\n",
    "    if occupation:\n",
    "        new_row[6]=map_occupations(occupation)\n",
    "\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_profiler_data(input_file, properties, profiled_data, debug=False):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        participants=pickle.load(f)\n",
    "\n",
    "    parts_per_name=defaultdict(dict)\n",
    "    \n",
    "    values_per_name=defaultdict(dict)\n",
    "    \n",
    "    given_values_num=defaultdict(int)\n",
    "    for doc_id, doc_data in participants.items():\n",
    "        for part_id, part_data in doc_data.items():\n",
    "            name=''\n",
    "            if 'Name' not in part_data.keys() or not part_data['Name'].strip(): continue\n",
    "            name=part_data['Name'].strip()\n",
    "            \n",
    "            tuple_input=get_property_tuple(properties, part_data)\n",
    "            for index, ti in enumerate(tuple_input):\n",
    "                if ti.strip()!='':\n",
    "                    given_values_num[index]+=1\n",
    "\n",
    "            values=profiled_data[tuple_input]\n",
    "            \n",
    "            for index, t in enumerate(tuple_input):\n",
    "                if t!='':\n",
    "                    values[properties[index]]=[tuple([t, 1.0])]\n",
    "            \n",
    "            parts_per_name[name][part_id]=values\n",
    "    return parts_per_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute JS distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js(p, q):\n",
    "    p = np.asarray(p).astype(np.float)\n",
    "    q = np.asarray(q).astype(np.float)\n",
    "   # normalize\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = (p + q) / 2\n",
    "    return (entropy(p, m) + entropy(q, m)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(v1, v2, domain):\n",
    "    l=len(domain)\n",
    "    mapped1=[0]*l\n",
    "    mapped2=[0]*l\n",
    "    \n",
    "    for k,v in v1:\n",
    "        if k not in domain: print('key1', k, domain)\n",
    "        index=domain.index(k)\n",
    "        mapped1[index]=v\n",
    "    for k,v in v2:\n",
    "        if k not in domain: print('key2', k, domain)\n",
    "        index=domain.index(k)\n",
    "        mapped2[index]=v\n",
    "    return mapped1, mapped2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_js_divergences(c1, c2, properties):\n",
    "    divs=[]\n",
    "    for p in properties:\n",
    "        mapped1, mapped2 = map_values(c1[p], c2[p], prop_vals[p])\n",
    "        div=js(mapped1, mapped2)\n",
    "        divs.append(div)\n",
    "    return divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "def cluster_matrix_with_features(matrix, algorithm='ward', max_d=0.3, criterion='distance'):\n",
    "    merges = linkage(matrix, 'ward')\n",
    "    clusters = fcluster(merges, max_d, criterion=criterion)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_matrix(distances, eps=0.1, min_samples=1):\n",
    "    labels=DBSCAN(min_samples=min_samples, eps=eps, metric='precomputed').fit_predict(distances)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "        \n",
    "    return list(labels), n_clusters, n_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(candidates, properties, start_id, eps, agg='avg'):\n",
    "\n",
    "    # intitialize an empty matrix\n",
    "    num_cands=len(candidates.keys())\n",
    "    dist_matrix = np.zeros(shape=(num_cands, num_cands)) # Distances matrix\n",
    "    \n",
    "    # fill the matrix with similarity values\n",
    "    the_keys=list(candidates.keys())    \n",
    "    for index1, p1 in enumerate(the_keys):\n",
    "        for index2, p2 in enumerate(the_keys):\n",
    "            if index1<index2:\n",
    "                c1=candidates[p1]\n",
    "                c2=candidates[p2]\n",
    "                #print(index1, index2)\n",
    "                divs=compute_js_divergences(c1, c2, properties)\n",
    "                \n",
    "                if agg=='max':\n",
    "                    agg_div=max(divs)\n",
    "                    #min_div=min(divs)\n",
    "                else: # agg=avg\n",
    "                    agg_div=sum(divs)/len(divs)\n",
    "        \n",
    "                dist_matrix[index1, index2]=agg_div\n",
    "                dist_matrix[index2, index1]=agg_div\n",
    "                \n",
    "    # run clustering\n",
    "    clusters, n_clusters, n_noise = cluster_matrix(dist_matrix, eps=eps)\n",
    "    clusters_json={}\n",
    "    for index, part_id in enumerate(the_keys):\n",
    "        cluster_id=start_id+int(clusters[index])\n",
    "        clusters_json[part_id]=cluster_id\n",
    "    \n",
    "    new_start_id=start_id+n_clusters\n",
    "    \n",
    "    return clusters_json, new_start_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(data, properties, eps):\n",
    "    \n",
    "    clusters={}\n",
    "    start_id=1\n",
    "    for name, name_candidates in data.items():\n",
    "        new_clusters, new_start_id=compute_clusters(name_candidates, properties, start_id, eps)\n",
    "        #if name=='Hayden Mayes':\n",
    "        #    print(new_clusters)\n",
    "        clusters.update(new_clusters)\n",
    "        start_id=new_start_id\n",
    "    print(start_id)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare to run the reasoning baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harden_values(data, t):\n",
    "    new_data={}\n",
    "    for name, name_data in data.items():\n",
    "        new_data[name]={}\n",
    "        for part_id, part_data in name_data.items():\n",
    "            new_data[name][part_id]={}\n",
    "            for prop, vals in part_data.items():\n",
    "                for val, prob in vals:\n",
    "                    prob=float(prob)\n",
    "                    if prob>=t:\n",
    "                        new_data[name][part_id][prop]=val\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_per_value_tuple(data, props):\n",
    "    group_by_name_plus=defaultdict(set)\n",
    "    for name, name_data in data.items():\n",
    "        for part, participant in name_data.items():\n",
    "            the_key=[]\n",
    "            the_key.append(name)\n",
    "            for p in properties:\n",
    "                if p in participant.keys():\n",
    "                    the_key.append(participant[p])\n",
    "                else:\n",
    "                    the_key.append('')\n",
    "            tuple_key=tuple(the_key)\n",
    "            group_by_name_plus[tuple_key].add(part)\n",
    "    print(len(group_by_name_plus.keys()))\n",
    "    return group_by_name_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(merger, agg_data):\n",
    "    if merger=='exact':\n",
    "        system_json=cu.transform_to_json(agg_data)\n",
    "    else: # 'noclash'\n",
    "        new_data=cu.perform_merging(agg_data)\n",
    "        system_json=cu.transform_to_json(new_data)\n",
    "    return system_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_aggregation=False # if false, then we get the max value as long as it is above the threshold tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "merger='noclash'\n",
    "tau=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_eps=0.1 # 0.05 or 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_clusters(clusters, output_file):\n",
    "    with open(output_file, 'w') as w:\n",
    "        json.dump(clusters, w)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(data):\n",
    "    num_parts=0\n",
    "    for name, name_data in data.items():\n",
    "        num_parts+=len(name_data.keys())\n",
    "    print(num_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participants_input\n",
      "762\n",
      "504\n",
      "459 762\n",
      "participants_samefirstname\n",
      "762\n",
      "476\n",
      "419 762\n",
      "participants_samename\n",
      "762\n",
      "47\n",
      "26 762\n",
      "participants_samelastname\n",
      "762\n",
      "491\n",
      "426 762\n"
     ]
    }
   ],
   "source": [
    "for f in glob.glob('%s/*.p' % input_dir):\n",
    "    filename=(f.split('/')[-1]).split('.')[0]\n",
    "    #if filename=='participants_samename': continue\n",
    "    print(filename)\n",
    "    output_file='%s/%s.json' % (output_dir, filename)\n",
    "    data=prepare_profiler_data(f, properties, profiled_data, debug=True)\n",
    "    inspect(data)\n",
    "    if soft_aggregation:\n",
    "        clusters=perform_clustering(data, properties, clustering_eps)\n",
    "    else:\n",
    "        rounded_data=harden_values(data, tau)\n",
    "        #less_props=['ethnic group', 'cause of death', 'sex or gender', 'religion', 'age group']\n",
    "        agg_data=aggregate_per_value_tuple(rounded_data, properties)\n",
    "        clusters=run_baseline(merger, agg_data)\n",
    "    \n",
    "    store_clusters(clusters, output_file)\n",
    "    print(max(clusters.values()), len(clusters.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'native language': [('English', 0.9999257963166346), ('French', 7.366188608406273e-05), ('German', 2.8576194336101364e-07)], 'ethnic group': [('White/Caucasian', 0.882382790768442), ('African American/Black', 0.1176172092315581), ('', 1.6218019968179536e-19)], 'religion': [('Christianity', 0.9991272093063862), ('atheism', 0.0007933334295516191), ('Judaism', 7.938017073685977e-05)], 'member of political party': [('Democratic Party', 0.6124247470056865), ('Republican Party', 0.3875752529943136), ('', 7.575389760492456e-22)], 'occupation': [('actor', 0.9097758303083633), ('journalist', 0.06358057310957367), ('singer', 0.01377762208040168)], 'cause of death': [('Suicide', 1.0)], 'sex or gender': [('male', 1.0)], 'age group': [('teen 12-17', 1.0)]} Marsavious Smith\n"
     ]
    }
   ],
   "source": [
    "for name, nd in data.items():\n",
    "    if '94412534e0b2172a6f7338f43290d772' in nd.keys():\n",
    "        print(nd['94412534e0b2172a6f7338f43290d772'], name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
