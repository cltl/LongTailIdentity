{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Prepare to run\n",
    "\n",
    "#### 1.1 Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import existing ones\n",
    "import csv\n",
    "import regex as re\n",
    "from collections import defaultdict, Counter\n",
    "import en_coref_md\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules we created\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Set data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: set which partition to work with!\n",
    "which_partition='full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SemEval input files (annotations and text)\n",
    "\n",
    "# annotation data\n",
    "se_annotation_dir='../data/input/%s/annotation' % which_partition\n",
    "se_annotation_file='%s/participants_input.p' % se_annotation_dir\n",
    "\n",
    "# text documents\n",
    "se_partial_text_file='../data/input/partial/text/docs.p'\n",
    "se_all_documents_path='../data/input/full/text'\n",
    "\n",
    "# SemEval output paths\n",
    "se_output_dir='../data/tmp/extracted_data/%s' % which_partition\n",
    "se_output_file='%s/extracted_data.p' % se_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GVDB data file\n",
    "gvdb_articles_file = '../../gvdb-aggregated-db/Articles-with-extracted-info.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Load lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emiel_resources_file='../resources/emiel.json'\n",
    "emiel_resources=utils.load_json(emiel_resources_file)\n",
    "\n",
    "students_resources_file='../resources/students.json'\n",
    "students_resources=utils.load_json(students_resources_file)\n",
    "\n",
    "genders_resources_file='../resources/genders.json'\n",
    "genders=utils.load_json(genders_resources_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Load coreference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = en_coref_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Define mapping of properties (lower-to-capitalletter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_properties ={\n",
    "                    'age': 'Age', \n",
    "                    'ethnicity': 'Ethnicity',\n",
    "                    #'occupation': 'Occupation',\n",
    "                    'causeofdeath': 'CauseOfDeath',\n",
    "                    'religion': 'Religion',\n",
    "                    'educationlevel': 'EducationLevel',\n",
    "                    'pastconviction': 'PastConviction',\n",
    "                    'residence': 'Residence',\n",
    "                    'birthplace': 'BirthPlace',\n",
    "                    'gender': 'Gender'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_attributes=list(map_properties.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Define how to process which attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process based on patterns and keywords\n",
    "pattern_attrs=['age','ethnicity', 'religion', 'educationlevel', \n",
    "               'causeofdeath', 'pastconviction', 'birthplace', 'residence']\n",
    "\n",
    "# Process based on coreference\n",
    "coref_attrs=['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Prepare the lexicons per attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Prepare pattern keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_data=defaultdict(dict)\n",
    "for k,v in students_resources.items():\n",
    "    new_v=utils.remove_stopwords(v)\n",
    "    pattern_data[k.lower()]=new_v\n",
    "\n",
    "#pattern_data['occupation']={o:o for o in emiel_resources['occupation-or-social-group']}\n",
    "\n",
    "#pattern_data['ethnicity'] |= set(emiel_resources['ethnicity'])\n",
    "#pattern_data['ethnicity'] |= set({'black', 'latino', 'white', 'hispanic', 'asian', 'latina', \n",
    "#       'african american', 'filipino', 'african-american', 'latinos',\n",
    "#      'palestinian', 'chinese-american', 'blacks', 'german-iranian'})\n",
    "\n",
    "\n",
    "#religions={o:o for o in (emiel_resources['religion']+students_resources['Religion'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 Prepare patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exact patterns per attribute\n",
    "patterns={'age': [r'\\d\\d?-year-old', r', \\d\\d?']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr, keyword_json in pattern_data.items(): \n",
    "    attr_patterns=set()\n",
    "    keywords=set(keyword_json.keys())\n",
    "    for keyword in keywords:\n",
    "        attr_patterns.add(r'\\b%s\\b' % keyword)\n",
    "    patterns[attr]=attr_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attribute value extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Proximity based extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attr_extractor_proximity(text, people_spans, coref_spans, sentence_offsets, patterns, a_dict=None):\n",
    "    \"\"\"Generic extractor that operates based on patterns.\"\"\"\n",
    "    \n",
    "    extracted_pairs=defaultdict(list)\n",
    "    for pattern in patterns:\n",
    "        r=re.compile(pattern, re.IGNORECASE)\n",
    "        values=r.finditer(text)\n",
    "        for val_found in values:\n",
    "            span=val_found.span()\n",
    "            value=val_found.group()\n",
    "            value=value.replace('-year-old', '').replace(',', '').strip()\n",
    "            if a_dict and value in a_dict:\n",
    "                value=a_dict[value]\n",
    "            person, distance=utils.find_closest_person(span, \n",
    "                                                       people_spans, \n",
    "                                                       coref_spans, \n",
    "                                                       sentence_offsets,\n",
    "                                                       min_dist=1000)\n",
    "            if person:\n",
    "                extracted_pairs[person].append(tuple([distance, value]))\n",
    "    clean_pairs=utils.get_closest_value_per_person(extracted_pairs)\n",
    "    return clean_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Coreference based extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attr_extractor_coref(clusters, names, values_json, debug=False): #, text, people_spans):\n",
    "    \n",
    "    person_data=defaultdict(list)\n",
    "\n",
    "    if not clusters:\n",
    "        return person_data\n",
    "    \n",
    "    for c in clusters:\n",
    "        mentions=utils.stringify_cluster_mentions(c.mentions)\n",
    "        for person_name in names:\n",
    "            if utils.lookup_person_in_list(person_name, mentions):\n",
    "                for m in c.mentions:\n",
    "                    for txt in [m.text, m.lemma_]:\n",
    "                        if txt.lower() in values_json.keys():\n",
    "                            person_data[person_name].append(values_json[txt.lower()])\n",
    "                            \n",
    "    clean_data={}\n",
    "    for person_name, gs in person_data.items():\n",
    "        c=Counter(gs).most_common(1)[0][0]\n",
    "        clean_data[person_name]=c\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Run extraction of all properties for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_properties(names, full_text, nlp):\n",
    "    \n",
    "    doc = nlp(full_text)\n",
    "    # run coreference\n",
    "    people_spans, coref_spans, clusters=utils.get_coref_spans(names, full_text, doc)\n",
    "    \n",
    "    sentence_offsets=utils.get_sentence_offsets(doc)\n",
    "    \n",
    "    all_extracted={}\n",
    "    for attribute in pattern_attrs:\n",
    "        all_extracted[attribute]=attr_extractor_proximity(full_text, \n",
    "                                                           people_spans, \n",
    "                                                           coref_spans, \n",
    "                                                           sentence_offsets, \n",
    "                                                           a_dict=pattern_data[attribute],\n",
    "                                                           patterns=patterns[attribute])\n",
    "    \n",
    "    all_extracted['gender']=attr_extractor_coref(clusters, names, genders)\n",
    "    \n",
    "    #combine extractors\n",
    "    combined=utils.singularize_data(all_extracted)\n",
    "            \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Peter Boer': {'age': '26',\n",
       "  'ethnicity': 'white/caucascian',\n",
       "  'religion': 'christian',\n",
       "  'causeofdeath': 'intentional',\n",
       "  'residence': 'texas'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test stuff\n",
    "\n",
    "txt='Hello Peter Boer, 26 was shot in church from Houston. The white police guy failed.'\n",
    "names=['Peter Boer']\n",
    "extract_properties(names, txt, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run GVDB extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_gvdb_data(the_file, limit=200):\n",
    "\n",
    "    all_gold_rows=[]\n",
    "    all_sys_rows=[]\n",
    "\n",
    "    with open(the_file, 'r') as csvfile:\n",
    "        rdr = csv.reader(csvfile, delimiter='\\t', quotechar='\"')\n",
    "        header=next(rdr)\n",
    "        \n",
    "        for c, row in enumerate(rdr):\n",
    "            if c==limit: break\n",
    "\n",
    "            full_text=row[2]\n",
    "            data=json.loads(row[3])\n",
    "            part_info=utils.get_participant_info(data)\n",
    "\n",
    "            names=set(part_info.keys())\n",
    "            if not len(names): continue\n",
    "\n",
    "            system_data = extract_properties(names, full_text, nlp)\n",
    "            all_sys_rows.append(system_data)\n",
    "            all_gold_rows.append(part_info)\n",
    "            \n",
    "            c+=1\n",
    "    return all_sys_rows, all_gold_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_sys_rows, all_gold_rows = process_gvdb_data(gvdb_articles_file, limit=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run SE extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': ' Ryan Morales', 'Age': ' 2', 'Age Group': ' Child 0-11', 'Gender': ' Male', 'Status': ' Killed', 'Type': ' Victim', 'DeathPlace': 'Texas', 'DeathDate': '2017'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "annotations=utils.load_pickle(se_annotation_file)\n",
    "print(annotations['2b10cc753152f0edaacf76314ab6ceec']['6306e4f5f77791a77e9bd9ea3efc9f17'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_se_data(annotation_file, text_file):\n",
    "\n",
    "    annotations=utils.load_pickle(annotation_file)\n",
    "    all_texts_json=utils.load_pickle(text_file)\n",
    "    #print(annotations['2b10cc753152f0edaacf76314ab6ceec']['6306e4f5f77791a77e9bd9ea3efc9f17'])\n",
    "    \n",
    "    all_gold_rows=[]\n",
    "    all_sys_rows=[]\n",
    "        \n",
    "    storable = {}\n",
    "     \n",
    "    for doc_id, part_data in annotations.items():\n",
    "        storable[doc_id]={}\n",
    "        \n",
    "        names=utils.get_names(part_data)\n",
    "        if doc_id not in all_texts_json:\n",
    "            text=utils.load_text_from_json('%s/%s.json' % (se_all_documents_path, doc_id))\n",
    "        else:\n",
    "            text_json=all_texts_json[doc_id]\n",
    "            conll_data=text_json['content']\n",
    "            text=utils.conll_to_text(conll_data)\n",
    "        \n",
    "        properties=extract_properties(names, text, nlp)\n",
    "        all_sys_rows.append(properties)\n",
    "             \n",
    "        for part_id, a_part_info in part_data.items():\n",
    "            \n",
    "            if 'Name' not in a_part_info.keys() or not a_part_info['Name']: \n",
    "                print(doc_id, part_id, a_part_info.keys())\n",
    "                input('continue?')\n",
    "                continue\n",
    "\n",
    "            name=a_part_info['Name'].strip()\n",
    "\n",
    "            these_properties={}\n",
    "            if name in properties.keys():\n",
    "                for k,v in properties[name].items():\n",
    "                    these_properties[map_properties[k]]=v\n",
    "\n",
    "            these_properties['Name']=name\n",
    "            storable[doc_id][part_id]=these_properties\n",
    "\n",
    "        some_info=utils.transform_part_info(part_data)\n",
    "        all_gold_rows.append(some_info)\n",
    "        \n",
    "        \n",
    "    return all_sys_rows, all_gold_rows, storable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_system, se_gold, data_to_store = process_se_data(se_annotation_file, \n",
    "                                                    se_partial_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'age': 11845,\n",
       "             'age group': 13218,\n",
       "             'gender': 12958,\n",
       "             'status': 13308,\n",
       "             'type': 13382,\n",
       "             'deathplace': 6026,\n",
       "             'deathdate': 6026,\n",
       "             'relationship': 949})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.count_per_attribute(se_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Benchmark extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Benchmark GVDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gvdb_attributes=['age', 'race', 'gender']\n",
    "# utils.benchmark_extractors(all_sys_rows, all_gold_rows, gvdb_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Benchmark on SemEval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'age' in se_attributes:\n",
    "    se_system=utils.map_age_to_group(se_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 sys white\n",
      "285 sys african american\n",
      "431 sys white/caucascian\n",
      "481 sys hispanic/latin\n",
      "482 sys white/caucascian\n",
      "544 sys african american\n",
      "548 sys african american\n",
      "591 sys african american\n",
      "727 sys white\n",
      "727 sys white\n",
      "950 sys african american\n",
      "1007 sys white/caucascian\n",
      "1172 sys african american\n",
      "1204 sys african american\n",
      "1205 sys african american\n",
      "1329 sys african american\n",
      "1443 sys african american\n",
      "1500 sys african american\n",
      "1577 sys white/caucascian\n",
      "1784 sys white/caucascian\n",
      "1888 sys african american\n",
      "1967 sys african american\n",
      "2209 sys african american\n",
      "2372 sys african american\n",
      "2558 sys african american\n",
      "2559 sys african american\n",
      "2560 sys african american\n",
      "2669 sys african american\n",
      "2671 sys african american\n",
      "2759 sys african american\n",
      "2900 sys african american\n",
      "2929 sys african american\n",
      "2941 sys african american\n",
      "2972 sys white\n",
      "2993 sys white\n",
      "3155 sys african american\n",
      "3157 sys african american\n",
      "3210 sys african american\n",
      "3263 sys white/caucascian\n",
      "3289 sys white/caucascian\n",
      "3410 sys african american\n",
      "3425 sys african american\n",
      "3442 sys african american\n",
      "3857 sys white\n",
      "3858 sys white\n",
      "3858 sys white\n",
      "3859 sys white\n",
      "4248 sys white\n",
      "4317 sys african american\n",
      "4324 sys african american\n",
      "defaultdict(<class 'int'>, {'gender': 851}) defaultdict(<class 'int'>, {'causeofdeath': 1915, 'residence': 1554, 'age': 3266, 'educationlevel': 245, 'pastconviction': 985, 'birthplace': 21, 'ethnicity': 50, 'gender': 89, 'religion': 37}) defaultdict(<class 'int'>, {'age': 11845, 'gender': 12107})\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-86961087961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark_extractors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mse_system\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_gold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ethnicity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/phd/ProfilingTask/LongTailIdentity/EntityIdentity/baselines/utils.py\u001b[0m in \u001b[0;36mbenchmark_extractors\u001b[0;34m(system, gold, attributes, debug)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mprec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "utils.benchmark_extractors(se_system, se_gold, se_attributes, debug='ethnicity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare SE output to be run by the baselines\n",
    "\n",
    "Desired format: pickle\n",
    "\n",
    "```\n",
    "{\n",
    "    doc_id:\n",
    "    {\n",
    "        part_id:\n",
    "        {\n",
    "            prop: value, \n",
    "            prop2: value2,\n",
    "            ...\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Process the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.dump_pickle(data_to_store, se_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Process the altered versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/input/full/annotation/participants_input.p ../data/input/full/annotation/participants_input.p\n",
      "not found number: 0\n",
      "extracted_data/full/participants_input.p\n",
      "../data/input/full/annotation/participants_samefirstname.p ../data/input/full/annotation/participants_input.p\n",
      "not found number: 0\n",
      "extracted_data/full/participants_samefirstname.p\n",
      "../data/input/full/annotation/participants_samename.p ../data/input/full/annotation/participants_input.p\n",
      "not found number: 0\n",
      "extracted_data/full/participants_samename.p\n",
      "../data/input/full/annotation/participants_samelastname.p ../data/input/full/annotation/participants_input.p\n",
      "not found number: 0\n",
      "extracted_data/full/participants_samelastname.p\n"
     ]
    }
   ],
   "source": [
    "not_found_data=defaultdict(set)\n",
    "for f in glob.glob('%s/*.p' % se_annotation_dir):\n",
    "    #if f.strip()!=se_annotation_file:\n",
    "    print(f, se_annotation_file)\n",
    "    altered_data=utils.load_pickle(f)\n",
    "\n",
    "    not_found=0\n",
    "    new_data=defaultdict(dict)\n",
    "    for doc_id, doc_data in altered_data.items():\n",
    "        doc_property_data=data_to_store[doc_id]\n",
    "\n",
    "        for part_id, part_data in doc_data.items():\n",
    "            if part_id not in doc_property_data.keys():\n",
    "\n",
    "                #print('NOT FOUND IN THE EXTRACTED DATA. document:', doc_id, '; participant:', part_id)\n",
    "                not_found+=1\n",
    "                not_found_data[doc_id].add(part_id)\n",
    "                continue\n",
    "\n",
    "            new_part_data={}\n",
    "            if 'Name' in part_data:\n",
    "                new_part_data['Name'] = part_data['Name']\n",
    "            for k, v in doc_property_data[part_id].items():\n",
    "                if k!='Name':\n",
    "                    new_part_data[k]=v\n",
    "            new_data[doc_id][part_id]=new_part_data\n",
    "\n",
    "    print('not found number:', not_found)\n",
    "    new_file_path='%s/%s' % (se_output_dir, f.split('/')[-1])\n",
    "    print(new_file_path)\n",
    "\n",
    "    utils.dump_pickle(new_data, new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug missing participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id='9897ff64ff1c41541dd9c4bdb3e2026b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_store[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2inc_file='../data/tmp/auxiliary_data/doc2inc.p'\n",
    "with open(doc2inc_file, 'rb') as f:\n",
    "    doc2inc=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2inc[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
